# End-to-End Clinical Heart Disease Prediction using Machine Learning Models
end-to-end Machine Learning project focused on Heart Disease Prediction using clinical patient data and advanced model evaluation techniques.

## ğŸ” Project Objective:
 The goal of this project was to predict the presence of heart disease and perform a comparative analysis of multiple classification models to identify the best-performing algorithm based on robust evaluation metrics.

## ğŸ“Š Dataset & Preprocessing:
 I used the UCI Heart Disease Dataset and applied several preprocessing steps including:
 â€¢ Handling missing values using imputation
 â€¢ One-hot encoding categorical features
 â€¢ Feature scaling with StandardScaler

## ğŸ“Š Machine Learning Model Evaluation Dashboard

An interactive Streamlit-based dashboard for evaluating and comparing multiple classification models using key performance metrics such as Accuracy, Weighted F1-score, Confusion Matrix, ROC-AUC, and Precisionâ€“Recall.

This project focuses on model evaluation, error analysis, and performance interpretation, making it highly valuable for Data Scientist and Machine Learning Engineer portfolios.

## ğŸ” Project Overview

Choosing the right machine learning model is not just about accuracyâ€”it requires understanding how models behave, where they fail, and how they trade off precision and recall.

### This dashboard provides:

Side-by-side comparison of multiple ML classifiers

Confusion matrix visualizations

Automatic best-model identification

Clear metric-based decision support

## ğŸ§  Models Evaluated

Logistic Regression

Decision Tree

Random Forest

K-Nearest Neighbors (KNN)

Support Vector Machine (SVM)

XGBoost

Naive Bayes

Gradient Boosting

AdaBoost

Extra Trees

## ğŸ› ï¸ Tech Stack

Python

Streamlit

Pandas

NumPy

Scikit-learn

Matplotlib

## âœ¨ Key Features

ğŸ“Š Interactive Streamlit dashboard

ğŸ“‹ Model comparison table (Accuracy & Weighted F1-score)

ğŸ§© Confusion matrix visualization for multiple models

ğŸ† Automatic best-performing model detection

ğŸ¨ Clean UI with custom CSS styling

ğŸ“± Responsive wide-layout design

## ğŸ“Š Model Performance Summary

The dashboard displays a comparison table containing:

Accuracy

Weighted F1-score

These metrics help identify models that balance class-level performance rather than favoring dominant classes.

## ğŸ§© Confusion Matrix Analysis

Confusion matrices are visualized for selected models to analyze:

True Positives

True Negatives

False Positives

False Negatives

This helps in understanding:

Which classes are misclassified

Model bias toward specific classes

Error distribution patterns

## ğŸ“ˆ ROC-AUC Explained

ROC-AUC (Receiver Operating Characteristic â€“ Area Under Curve) measures how well a model can distinguish between classes across different probability thresholds.

### Why ROC-AUC Matters:

Threshold-independent metric

Measures overall class separability

Higher AUC = better model discrimination

## Interpretation:

AUC = 0.5 â†’ No discrimination (random guessing)

AUC = 1.0 â†’ Perfect classification

ROC-AUC is especially useful when:

Class imbalance exists

Probability-based predictions are required

## ğŸ“‰ Precisionâ€“Recall Explained

Precisionâ€“Recall Curve focuses on the performance of the positive class, making it critical for imbalanced datasets.

## Key Metrics:

Precision â†’ How many predicted positives are actually positive

Recall â†’ How many actual positives were correctly identified

## Why Precisionâ€“Recall is Important:

More informative than ROC-AUC for rare classes

Highlights trade-off between false positives and false negatives

ğŸ“Œ In real-world use cases (fraud detection, churn prediction, medical diagnosis), Precisionâ€“Recall often matters more than accuracy.

## ğŸ† Best Model Selection

The dashboard automatically identifies the best-performing model based on:

Highest Accuracy

Strong Weighted F1-score

This helps users quickly select a production-ready model while still analyzing trade-offs visually.

## â–¶ï¸ How It Works

Model metrics are stored in a structured dataset.

Accuracy & F1-scores are displayed in a comparison table.

Confusion matrices are plotted for selected models.

Best model is highlighted dynamically.

Users interpret results using multiple evaluation perspectives.

## â–¶ï¸ Usage

Run the dashboard locally using:

streamlit run app.py

## ğŸ“Œ Output

The dashboard provides:

Model comparison table

Confusion matrix visualizations

Best model summary

Clear performance interpretation
